module m_mpi_halo_exchange_j
contains
   subroutine mpi_halo_exchange_j(f)
#ifdef MPI
      use mpi, only : MPI_Irecv, MPI_Isend, MPI_Waitall, MPI_COMM_WORLD, MPI_STATUS_SIZE
      use m_mpi_pack_jplane
      use m_mpi_unpack_jplane
      use m_mpi_halo_buffers
#endif
#ifdef _CUDA
      use cudafor
#endif
      implicit none
      real :: f(:,:,:,:)
#ifdef _CUDA
      attributes(device)  :: f
#endif
      integer :: ierr, req(4), stat(MPI_STATUS_SIZE,4)

#ifdef _CUDA
      type(dim3) :: B, G
#endif
      integer istat
      print *,mpi_rank,' in mpi_halo_exchange_j'

#ifdef _CUDA
      B = dim3(16,8,4)
      G = dim3( ((nx+2)+B%x-1)/B%x, ((nz+2)+B%y-1)/B%y, (nl+B%z-1)/B%z )
#endif

      ! Pack j=1 and j=ny planes
      print *,mpi_rank,' Pack j=1 planes'
      call mpi_pack_jplane&
#ifdef _CUDA
      &<<<G,B>>>&
#endif
      &(f, 1,          snd_south)


      print *,mpi_rank,' Pack j=ny planes'
      call mpi_pack_jplane&
#ifdef _CUDA
      &<<<G,B>>>&
#endif
      &(f, ny,   snd_north)

#ifdef _CUDA
      istat=cudaDeviceSynchronize()
#endif

      print *,mpi_rank,' receives'
      ! Nonblocking CUDA-aware MPI: south uses tags 101/102, north mirrored
      call MPI_Irecv(rcv_south, plane_elems, MPI_REAL, south, 102, MPI_COMM_WORLD, req(1), ierr)
      call MPI_Isend(snd_south, plane_elems, MPI_REAL, south, 101, MPI_COMM_WORLD, req(2), ierr)
      call MPI_Irecv(rcv_north, plane_elems, MPI_REAL, north, 101, MPI_COMM_WORLD, req(3), ierr)
      call MPI_Isend(snd_north, plane_elems, MPI_REAL, north, 102, MPI_COMM_WORLD, req(4), ierr)

      call MPI_Waitall(4, req, stat, ierr)
      print *,mpi_rank,' receives done'

! Unpack into ghosts j=0 and j=ny+1
      call mpi_unpack_jplane&
#ifdef _CUDA
      &<<<G,B>>>&
#endif
      &(f, 0,            rcv_south)

      call mpi_unpack_jplane&
#ifdef _CUDA
      &<<<G,B>>>&
#endif
      &(f, ny+1,   rcv_north)

#ifdef _CUDA
      istat=cudaDeviceSynchronize()
#endif

   end subroutine
end module

